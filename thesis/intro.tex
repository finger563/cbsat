% $Id: intro.tex 3799 2013-10-14 16:13:38Z emfinger $
\chapter{Introduction}
\label{ch:intro}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Motivation}
\label{sec:motivation}

Cyber-Physical Systems (CPS) are defined as co-engineered interacting
networks of physical and computational components.  These classes of
systems consist of computers running sensing and actuation control
software to interact closely with a physical system in a physical
environment.  In these systems, the computer controls some or multiple
aspects of the physical system; this control aspect tightly couples
the software with the computer's physical environment.  Because of
this tightly coupled nature, the software not only affects the
computer and its surrounding environment, but also is affected by the
surrounding environment.  Many different types of systems fit this CPS
description: e.g. autonomous vehicles including Unmanned Aerial
Vehicles, Unmanned Underwater Vehicles, autonomous cars, satellites,
extra-terrestrial rovers, and embedded or wireless sensors/actuators.

Consider a quadcopter and its flight control software.  The flight
control software must monitor the orientation and positioning sensors
of the quadcopter, estimate the state based on the sensor data, and
then calculate control outputs to drive the system to a goal state.
Such a system deals with many constraints, ranging from weight, power,
and size constraints, to processor speed, memory capacity, and
wireless operational range constraints.  Despite these constraints,
the flight control software must complete the
sense-estimate-calculate-actuate loop within enough time to ensure
that the quadcopter's state does not transition into an unstable
state.  Such an unstable state might be the quadcopter flipping over
or crashing into an obstacle.  The dynamics of the control system,
i.e. the physics of the quadcopter's motion and its control input
response, define the bounds on the time the flight control software
can spend in each iteration of the control loop.  If the sensing, or
state estimation tasks of the control loop take too long to complete,
the state estimated may no longer be accurate and thus the control
outputs may send the quadcopter into an unstable state or the
quadcopter may have transitioned into an unstable state.

\iffalse
As an example, consider a satellite
and its control software.  The control software reads the sensor data
from the satellite's sensors to determine the current state vector of
the satellite.  Using this state information, the software's feedback
control loop governing the orbital position of the satellite may
determine that a thruster on the satellite must be activated to
correct the satellite's orbit, e.g. orbit degradation caused by
atmospheric friction or gravitational perturbations.  The satellite
has very limited resources, both with respect to physical resources
like propellant, as well as computing resources, like power and
processor time.  Because of these resource constraints, if the
software component involved with determining the state of the system
does not meet its timing deadlines and instead calculates the
satellite state too late (e.g. high computational load causing the
Kalman-filter state estimation to miss its deadline), the satellite's
orbit correction may be too late.  Similarly, the software component
involved with activating the satellite's thruster must meet its
deadlines otherwise the satellite will not achieve the proper orbit.
\fi

As these types of CPS are being scaled-up, they are becoming more
distributed in nature.  The systems mentioned above could scale up to
unmanned swarms of search and rescue drones, for instance, or large
sensor/actuator networks for power distribution and control. Because
each subsystem can directly and indirectly affect the others, all
subsystems must communicate their states to each other, closing the
control loop through the network.

An example of such scaling up is the recent research into developing
fractionated spacecraft\cite{fractionated_spacecraft}.  A fractionated
spacecraft is a cluster of satellites cooperating, communicating and
running distributed applications in service of the mission goals.
Such a cluster design replacese the traditional monolithic satellites
which are more expensive to develop, deploy, repair, and are more
difficult to upgrade with new functionality.  Because of this trend
towards cooperating distribution of system resources, the network
facilitating the cooperation and communications becomes a critical
resource to the system.  Whereas the single satellite's internal
communications bus (direct physical connection system internal to a
single satellite which allows sensing and actuation controlled by the
computer) was ignored in the previous example, the wireless
communications network enabling the satellite cluster cannot be
ignored when analyzing the properties of the system.  Because the
satellites are expensive to deploy, impossible to repair, and must
last for a long time to satisfy both budgetary constraints and mission
goals, the application developers and system integrators for the
cluster must ensure that the software on the cluster does not
compromise its ability to meet the mission goals.  For instance, the
same orbit maintenance described previously now necessitates the use
of the cluster's network.  For a satellite to activate its thruster to
maintain or modify its orbit, it must first ensure that such an action
will not cause a collision with another of the satellites in the
cluster.  Therefore, every satellite must know the state of every
satellite in the system, and any thruster activation must be a
coordinated action to ensure the safety and continued operation of the
cluster.  All of this state distribution and coordination occurs over
the wireless network between the satellites, which (1) has limited
resources, (2) is shared between all applications on all the
satellites, and (3) varies as a function of time throughout the orbits
of the satellites according to the orbital mechanics defining the
system.  The equations of motion for the satellites define the orbital
paths taken by the satellites\cite{fundamentals_astrodynamics}.  These
paths are, for our purposes, cicular or elliptical orbits, where each
satellite in the clsuter has the same orbital period and speed.  Since
these orbits have the same period, the distances between the
satellites will vary periodically as a function of time.  Because the
satellites use a wireless network where the latency and bandwidth are
directly proportional to distance, the wireless network capacity of
the satellites will vary proprotionally with respect to time.  This
final point about the equations of motion and their effect on network
capacity is especially important, since it highlights how the physics
of the system directly and drastically affects system resources and
performance.  Again, we must ensure the timing requirements of the
control loops are met, except now those timing properties are directly
related to the network resources, e.g. the end-to-end latency of
traffic on the network links, the bandwidth of the links, and the
buffer space available to the applications on each satellite.

The system provides network resources to applications and users of the
system as a service.  The quality of this service as seen by the users
of the system is defined as the Quality of Service (QoS) of the
network and is the overall performance of the network as seen by its
users.  The specific aspects of QoS which we focus on are the network
bandwidth, end-to-end network latency, and availability of the network
resources.  For critical systems such as those described above which
may be quite difficult to repair or replace, such requirements must be
analyzed at design-time and verified to ensure that they are met.  In
any distributed CPS, the network performance of the system is affected
by the physical environment of the system as it affects the network.

For systems using wired networks, the delay caused by the networks can
affect the performance of the control systems.  Further, during
periods of high network load, the network performance as seen by the
application traffic will degrade, which can increase the latency and
buffer space required by the applications.  Therefore, even in wired
networks, analyzing the affect of applications' network traffic on
each other is important for understanding the quality of the network
service as seen by the applications.  For systems whose physical
network layer is made up of wireless connections, the physical
environment has an even larger effect on the network resources and
availability.  Environmental interference or obstruction leading to
multi-path self-interference or signal degradation can combine with
the distance-based signal-to-noise ratio loss due to the nature of
wireless media.  Such effects can induce hysteresis or instability in
control systems through loss of data on the network, or increased
variability in the response-time of the control loops.  Because the
network performance of such a system is so tightly coupled with the
physics governing the system, the physical dynamics must be taken into
account when predicting the run-time characteristics of the network.
Additionally, such resource constrained systems which are expensive to
develop and deploy must maximize their return on investment through
the hosting of payload applications (e.g. for scientific research),
while ensuring that the resource requirements are not exceeded.  This
design-time analysis of time-varying resources and their constraints
is paramount to ensuring a stable system, where we define stability
here as 1) all applications have finite bounded network resource
requirements, 2) All application's network resource requirements can
be satisfied by the system.  Such stability means that applications'
data will be serviced by the system without loss and within the time
required by the application.  For networked distributed control
systems, this definition of stability with respect to the network is
required for meeting stability requirements of the control system
itself.  

Incorporating the physical dynamics into the model of the system
network resources addresses only half of the problem, however.  To
facilitate accurate, meaningful resource constraint analysis, the
application developers are expected to model and describe the resource
and timing constraints of their applications.  As stated above, many
of these systems have long-term missions, for which simple, static
minimum/maximum resource and timing requirements lead to inefficient,
underutilized, over-specified systems.  To increase the fidelity of
the application resource utilization model with respect to the actual
application's resource usage, the time-varying nature of the
application's network utilization should be modeled.  In this way,
tighter bounds on performance characteristics and resource utilization
can be achieved.  Tighter bounds on application performance and
resource utilization allow system integrators to increase overall
system resource utilization to maximize the mission-specific or
scientific return of the system while still ensuring all applications
receive their required services.

In addition to the design-time modeling and analysis which facilitates
the calculation of performance guarantees about such critical CPS, the
run-time systems require monitoring and management of resources and
their utilization to prevent faulty or malicious applications from
causing resource over-utilization and possibly making the system
unstable or completely bringing the system down.  Often this resource
management is simply enforcing a static cap on resource utilization
for each application.  For such trivial resource management, often the
operating system or other platform infrastructure is used to enforce
these bounds on the applications' resources, e.g. open file descriptor
limits or maximum buffer size limits being enforced in the operating
system kernel.  However, higher fidelity design-time models which more
precisely capture the behavior and resource requirements of the
applications can allow more sophisticated, time-varying resource
monitoring and management.

Another type of adaptive resource management falls under the class of
self-adaptive systems, which are capable of self-management at
run-time.  Using recent developments in autonomic computing, systems
can use the sensors at their disposal to monitor their available
resources as well as their environment, estimate the current state of
the system, and use the available system actions to transition into a
new state\cite{ibmAutonomicComputing2003}.  A relevant example for
such an adaptive system would be to eschew the design-time network
modeling and analysis of what at run-time would be a relatively static
system in favor of an adaptive network which manages the network
resources for the applications based on the available resources the
system has.  Such a design has the benefits of possibly better
utilization of system resources and better resilience to unplanned or
unforeseen system events or states, but has the drawback of difficult
design-time analysis.  Currently, analyzing these adaptive systems at
design time to derive guarantees about system behavior, resource
availability, or performance is quite difficult and in many cases
infeasible.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Challenges}
\label{sec:challenges}
The systems described above face many challenges for network
performance prediction, as might be required by mission- or
safety-critical application developers.  Furthermore, an application
which consumes more resources than specified at design-time, either
through malicious or faulty code, can send these CPS into an unstable
state by starving critical control processes of resources.  Control
systems can be forced into unstable system states, for example by high
network latency exceeding the timing requirements of the control
loops.  Many systems may have a looser definition of stability,
i.e. defined by the application with respect to that application, but
we will consider loss of data or exceeding the latency requirements to
be unstable behavior. In this section we outline the main challenges
facing application developers and system integrators pertaining to
network performance prediction and management and we separate these
two classes of challenges into Design-Time Analysis challenges and
Run-Time Management challenges.

\subsection{Design-Time Network Performance Analysis of Distributed CPS Applications}
A principal challenge of system design is the performance analysis of
a system, its resources, and its applications at design-time.  Such
analysis and prediction is critical for remotely managed systems and
allows system integrators to provide guarantees to application
developers about the services provided by the system.  However, for
complex distributed cyber-physical systems such design-time analysis
is challenging.  Such analysis may require capturing the behavior of
the system and its applications in models that can then be composed
and analyzed.  Ensuring that the models properly capture the relevant
characteristics of the run-time system is a challenge by itself, and
is compounded by the challenge of composing the models for analysis.
Such challenges for design-time network performance analysis are
\begin{itemize}
\item Modeling the interaction of the system with the physical world
  is difficult, esp. with respect to how the interaction directly or
  indirectly affects system resources and performance.
\item Models of application network utilization can be imprecise and
  difficult to derive without a running system
\item Application models may not represent actual application traffic
  on the network due to implementation details such as transport
  protocol selection (e.g. UDP vs TCP), which may alter the required
  bandwidth or buffering latency.
\item Developing distributed applications for such systems is
  difficult, and should be done in a way that is amenable to modeling,
  analysis, and verification.
\item Infrastructural code which handles low level system functions or
  network communications may obscure the application's network
  behavior from the application developer, making modeling of the
  application's network requirements difficult
\item Network resources are becoming more critical to distributed CPS,
  but existing tools and techniques for design-time analysis of
  network resource utilization and performance do not support robust,
  precise analysis of such time-varying constraints
\item For resource constrained systems, no processor or memory
  resources should be wasted, but without accurate and precise
  design-time analysis, systems must conservatively over-approximate
  network resource requirements.
\item For application/system data flows in the network which require
  tight and/or real-time guarantees on temporal properties,
  design-time analysis is critical.
\item Most systems route network traffic for nodes which cannot
  directly communicate. These routes may be defined at design-time and
  remain constant for the duration of the sytem, or may be unknown at
  design-time, changing dynamically during the run-time of the system.
  Dynamic routing is difficult to analyze for precise performance
  prediction because the the routes used by traffic may be unknown at
  design-time.  
\end{itemize}

\subsection{Run-Time Network Resource Monitoring and Management}
Given specifications for system network resources and application
network resource requirements, the system must ensure that no
application either purposefully or inadvertently exceeds its allowed
resource limits and starves other applications or critical system
processes of those precious resources.  Such resource management is
crucial for ensuring system stability and proper service quality to
applications and end-users.  For systems with highly time-varying
application load, system resource availability, or both, static limits
under-utilize the system's resources.  For such systems, higher
fidelity resource management is needed to maximize the utilization of
the system's resources.  Further, these higher fidelity system and
application network resource models pave the way for more accurate and
robust failure or network attack (e.g. Denial of Service) detection
which in turn can provide higher system stability.  Challenges towards
the development of such run-time network resource management are
\begin{itemize}
\item Available network resources at run-time should not be wasted if
  applications can use them, but allowing run-time management is
  difficult because the behavior is difficult to analyze at
  design-time for performance analysis and prediction.
\item Anomalies caused by applications attempting (due to either
  faults or attacks) to use more network resources than they
  originally specified should be detected and mitigated; the detection
  of coordinated attacks, e.g. distributed denial of service (DDoS),
  requires more sophisticated detection and mitigation techniques
\item Systems are becoming more adaptive in nature and reacting to
  events at run-time (essentially data-dependent traffic); this
  adaptability is hard to provide performance metrics or guarantees
  for
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Organization}
\label{sec:organization}
The rest of this thesis is organized as follows
\begin{itemize}
\item Chapter~\ref{ch:background} describes the related work in
  network analysis and management of distributed applications
\item Chapter~\ref{ch:designTime} describes design-time network
  performance analysis and prediction for CPS applications
\item Chapter~\ref{ch:runTime} describes run-time network performance
  monitoring and management for CPS applications
\item Chapter~\ref{ch:conclusions} concludes the thesis and describes
  possible extensions to the work in the future
\item Appendix~\ref{ch:publications} lists the publications so far
\end{itemize}





%%% TEXT THAT IS NO LONGER NEEDED:

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\iffalse In our work, we developed analysis techniques for very
accurately and precisely predicting run-time application network
performance and resource utilization from design-time models of the
application network resource requirement profiles and system network
resource provision profiles.  We showed how these models could be used
to describe applications on wireless networks communicating using
shared channel protocols, e.g. TDMA, as well as other medium access
protocols, e.g. FDMA.  We experimentally validated these analysis
techniques using run-time network emulation to enforce the system
network resource profiles and application code which generated network
traffic according to the applications' network resource requirement
profiles.  Furthermore, we extended these analysis techniques to be
implemented in a middleware stack that provided anonymous
publish/subscribe and asynchronous/synchronous method invocation.  Our
middleware extension measured the data production of each application
and compared it against the application's network resource requirement
profile.  By comparing the stated resource requirements to the
application's actual resource utilization, we could detect deviations
from the supplied profile and take action based on some condition.
For our implementation, we assumed any traffic which violated the
supplied profile was anomalous and therefore we discarded it before it
made it into kernel space.  Finally, we developed a component model
for Robot Operating System (ROS) middleware, into which we are
integrating our monitoring and analysis techniques.  We plan on using
this infrastructure and our distributed Resilient CPS testbed to
perform application and system network modeling and analysis tests,
such as network anomaly classification.  We hope our modeling and
analysis techniques and tools will be useful for designing, verifying,
and deploying resilient distributed CPS.

Cyber-Physical Systems (CPS) are becoming increasingly more
distributed in nature, following the trend towards the internet of
things (IoT) devices.  These distributed systems interact closely with
the physical world and require the use of communications channels
between the hardware nodes of the system as well as to external
systems.  Since such systems are generally remotely deployed and
managed, applications which are deployed onto the systems must be
analyzed and verified in some way to ensure that the system can
provide the application's required services and that the application
will not degrade the overall system's functionality.  As these systems
become more distributed in nature and rely more heavily on the network
for communications, the network utilization and resources are becoming
larger factors in the analysis and verification of distributed CPS and
their applications.

Many CPS applications require networking of some form in order for the
system to function nominally.  This networking often performs a key
role in the system, such as facilitating the communication and control
of distributed sensors and control systems.  Traditionally, these
networks of CPS have been both isolated from external influences and
predefined at system design-time.  This isolation and
pre-determination creates a static network with respect to both the
topology of the network and the capacity of each network link.  More
recently however, CPS have become less isolated and more dynamic by
utilizing heterogeneous and wireless networks and incorporating
mobility.
%Additionally, high-criticality or mixed-criticality systems require that the design and analysis ensure a minimum of failures once the system has been deployed.

%As design and analysis tools as well as the deployment systems grow in capability, these static constraints are being relaxed in favor of more capable systems with mobile system nodes, whose communication network can have both dynamic topology and dynamic link capacity.  

Some wireless mobile CPS networks, such as the network between a
cluster of satellites orbiting Earth,
%vary with respect to time but 
vary periodically with respect to time, \emph{e.g.} according to the
cluster's orbital period.  For such networks, the physical dynamics of
the nodes in the cluster are well understood and predictable,
therefore the network dynamics can be fairly predictable as well.  For
such predictable or periodic dynamic networks, the use of worst-case
network performance for analysis and constraint verification wastes
the network resources over much of the lifecycle of the
system. Integrating the physical dynamics of the network into the
modeling and analysis tools improves the performance of the systems
without degrading its reliability.

%Currently, many cyber-physical systems being developed have the technical capability for dynamic networking, however the analysis and design tools often lack the ability to fully utilize this dynamic network.  High-criticality or mixed-criticality systems require that the design and analysis ensure a minimum of failures once the system has been deployed. Such strict analysis and verification requirements are at odds with variable networks.  


%To facilitate the design, analysis, and deployment of such critical, managed cyber-physical systems, we have developed an integrated design, analysis, and deployment toolsuite for \textbf{D}istributed \textbf{R}eal-Time \textbf{E}mbedded \textbf{M}anaged \textbf{S}ystems (\textbf{DREMS})\cite{ISIS_F6_Aerospace:12} \cite{DREMS13Software}. The DREMS platform consists of a run-time architecture leveraging an operating system with secure communication and task partitioning as well as middleware layers supporting application interface abstraction. Additionally, the design and analysis toolsuite facilitates component-based application and system design and integration.  This design toolsuite allows application developers to design distributed applications for cyber-physical systems which communicate through pre-defined interaction patterns.  The restriction of component interactions and behavior to predefined interaction patterns enables the generation of much of the \emph{glue code} which allows application components to communicate with each other.

However, our current design tools do not incorporate the physical
dynamics of the network for analysis of network constraints on the
applications.  Towards these goals, we have developed time-varying
network traffic models that can be specified by the application
developer and analyzed against the system network profiles, specified
as $[time,data]$ series. By analyzing these profiles, we can determine
exactly how the system will transfer the application data. Analysis of
this transmitted data profile vs. the application data profile
provides the developer with information about both the minimum buffer
required for application communication without loss of information and
the maximum buffer delay in communication caused by the network.
Metrics such as these allow the developer to guarantee in design time
that the application will not exceed its memory and latency
constraints at run-time.

\fi
