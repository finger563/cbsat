.. _results:

Results
=======

This page covers the results of my research as it applies to analysis
of networked CPS.  I will cover the research contributions in two
aspects:

* :ref:`design_time` : Details design-time network analysis
  contributions and improvements to network performance prediction
* :ref:`run_time` : Details the run-time network monitoring and
  management contributions which have been based of the design-time
  work

.. _design_time:

Design Time Results
-------------------

Precise Analysis for Deterministic Queuing Systems
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

To model the network capability of the system and the application
traffic patterns, we have developed a network modeling paradigm
similar to Network Calculus' traffic arrival curves and traffic shaper
service curves.

Similarly to Network Calculus' arrival curves and service curves, our
network profiles model how the network performance or application
traffic generation changes with respect to time.  Whereas Network
Calculus' modeling transforms application data profiles and network
service profiles into min and max curves for data received vs. size of
time-window, our models take a simpler, deterministic approach which
models exactly the data generated by the application and the data
which could be sent through the network, allowing our performance
metrics to be more precise.  Specifically, the bandwidth that the
network provides on a given communication link is specified as a time
series of scalar bandwidth values. Here, bandwidth is defined as data
rate, i.e. bits per second, over some averaging interval.  This
bandwidth profile can then be time-integrated to determine the maximum
amount of data throughput the network link could provide over a given
time.  The bandwidth profile for the application traffic similarly can
be time-integrated to determine the amount of data that the
application attempts to send on the network link as a function of
time.

Having time-integrated the bandwidth profiles to obtain data vs. time
profiles that the application requires and that the system provides,
we can use convolution (:math:`\otimes`) on these two profiles to
obtain the transmitted link data profile as a function of discrete
time. The convolution we define on these profiles borrows concepts
from the min-plus calculus used in Network Calculus, but does not use
a sliding-window and instead takes the transformed minimum of the
profiles. For a given application data generation profile,
:math:`r[t]`, and a given system link capacity profile :math:`p[t]`,
where :math:`t\in\mathbb{N}`, the link transmitted data profile
:math:`l[t]` is given by the convolution equation
:eq:`convolution`. The difference :math:`(p[t-1] - l[t-1])` represents
the difference between the amount of data that has been transmitted on
the link :math:`(l[t-1])` and the data that the link could have
transmitted at full utilization :math:`(p[t-1])`. As demonstrated by
the convolution equation, :math:`\forall t : l[t] \le r[t]`, which is
the relation that, without lower-layer reliable transport, the link
cannot transmit more application data for the application than the
application requests as there will be packetization and communication
header overhead as well.  The buffer and delay equations
:eq:`convolution` use the output of the convolution with the input
profile to predict the minimum required buffer size for lossless
tranmission and the maximum delay experienced by the transmitted data,
respectively.  A representative convolution example is shown below for
reference.

.. math::
   y=l[t] &= (r \otimes p)[t] \\
   &= min( r[t] , p[t] - (p[t-1] - l[t-1]) )\\
   \text{buffer}&= sup\{r[t] - l[t] : t \in \mathbb{N}\}\\
   \text{delay} &= sup\{l^{-1}[y]-r^{-1}[y] : y \in \mathbb{N}\}
   :label: convolution

.. figure:: /images/results/convolution.png
   :align: center

Given that the required data profile and system data service profile
are periodic, we must determine the periodicity of the output
profile.  If we can show that the output profile is similarly
periodic, then we can show that the system is stable.  First, let us
look at the profile behavior over the course of its first two periods
of activity.

We will examine two systems, *system (1)* and *system (2)*.  Firstly,
examine *(1)*, shown below (note: you can click on the images to open
them in a larger format):

+---------------------------------------------------+-----------------------------------------------------+
| System *(1)* Bandwidth for 1 Period               | System *(1)* Data for 1 Period                      |
+===================================================+=====================================================+
| .. image:: /images/results/1-period-system-bw.png | .. image:: /images/results/1-period-system-data.png |
|    :height: 200                                   |    :height: 200                                     |
+---------------------------------------------------+-----------------------------------------------------+

+---------------------------------------------------+-----------------------------------------------------+
| System *(1)* Bandwidth for 2 Periods              | System *(1)* Data for 2 Periods                     |
+===================================================+=====================================================+
| .. image:: /images/results/2-period-system-bw.png | .. image:: /images/results/2-period-system-data.png |
|    :height: 200                                   |    :height: 200                                     |
+---------------------------------------------------+-----------------------------------------------------+

We notice that for this example system, the second period output
profile is not an exact copy of the first (most easily seen by
examining the bandwidth plots), and yet the required buffer size is
still the same as it was when analyzing the system over one period.
Furthermore, by running the analysis over even larger number of
periods, we can determine (not plotted here for space and
readability), that the predicted buffer size does not change no matter
how many periods we analyze for this system.  Let us look at a system
where this is not the case before we begin the analysis of such system
characteristics.

+-----------------------------------------------------+-------------------------------------------------------+
| System *(2)* Bandwidth for 1 Period                 | System *(2)* Data for 1 Period                        |
+=====================================================+=======================================================+
| .. image:: /images/results/1-period-unstable-bw.png | .. image:: /images/results/1-period-unstable-data.png |
|    :height: 200                                     |    :height: 200                                       |
+-----------------------------------------------------+-------------------------------------------------------+

+-----------------------------------------------------+-------------------------------------------------------+
| System *(2)* Bandwidth for 2 Periods                | System *(2)* Data for 2 Periods                       |
+=====================================================+=======================================================+
| .. image:: /images/results/2-period-unstable-bw.png | .. image:: /images/results/2-period-unstable-data.png |
|    :height: 200                                     |    :height: 200                                       |
+-----------------------------------------------------+-------------------------------------------------------+

Notice in system *(2)*, the first period analysis predicted the same
buffer size as system *(1)*, but when analyzing two periods the
predicted buffer size changed.  Clearly the behavior of the system is
changing between these two periods.  If we continue to analyze more
periods of system *(2)*, as we did with system *(1)*, we'll find the
unfortunate conclusion that the predicted buffer size increases with
every period we add to the analysis.

We have discovered a system level property that can be calculated from
these profiles, but we must determine what it means and how it can be
used.  First, we see that in system *(1)*, the predicted required
buffer size does not change regarless of the number of periods over
which we analyze the system.  Second, we see that for system *(2)*,
the predicted required buffer size changes depending on how many
periods of activity we choose for our analysis window.  Third, we see
that the second period of system *(2)* contains the larger of the two
predicted buffer sizes.  These observations (with our understanding of
deterministic periodic systems) lead us to the conclusion: system
*(2)* can no longer be classified as periodic, since its behavior is
not consistent between its periods.  Furthermore, because the required
buffer size predicted for system system *(2)* continually increases,
we can determine that the system is in fact *unstable* due to
unbounded buffer growth.  

Let us now formally prove the assertion about system periodicity and
stability which have been stated above.  We will show that our
analysis results provide quantitative measures about the behavior of
the system and we will determine for how long we must analyze a system
to glean such behaviors. 

Consider a deterministic queuing system providing a data service
function :math:`S` to input data flow :math:`I` to produce output data
flow :math:`O`.  At any time :math:`t`, the amount of data in the
system's buffer is given by :math:`B_t`.  After servicing the input
flow, the system has a remaining capacity function :math:`R`.

* :math:`S[t]` : the service function of the system, data service
  capacity versus time
* :math:`I[t]` : the input data flow to the system, data versus time
* :math:`O[t]` : the output data flow from the system, data versus time
* :math:`B[t]` : the amount of data in the system's buffer at time
  :math:`t`, i.e. :math:`I[t]-O[t]`
* :math:`R[t]` : the remaining service capacity of the system after
  servicing :math:`I`, i.e. :math:`S[t] - O[t]`

Because :math:`S` and :math:`I` are deterministic and periodic, they
do not change from period to period, i.e. given the period :math:`T_I`
of :math:`I`, :math:`\forall t,n : I[t] = I[t + n*T_I]`.  Similarly,
given the period :math:`T_S` of :math:`S`, :math:`\forall t,n : S[t] =
S[t + n*T_S]`.

We can determine the hyperperiod of the system as the lcm of input
flow period and the service function period, :math:`T_p =
lcm(T_S,T_I)`.

At the start of the system, :math:`t=0`, the system's buffer is empty,
i.e.  :math:`B_0 = 0`.  Therefore, the amount of data in the buffer at
the end of the first period, :math:`t=T_p`, is the amount of data that
entered the system on input flow :math:`I` but was not able to be
serviced by :math:`S`.  At the start of the next period, this data
will exist in the buffer.  Consider the scenario that the system's
remaining capacity :math:`R` is less than the size of the buffer,
i.e. :math:`R[T_p] < B[T_p]`.  In this scenario, clearly,
:math:`B[2*T_p] > B[T_p]`, i.e. there will be more data in the buffer
at the end of the second period than there was at the end of the first
period.  Since the system is deterministic, for any two successive
periods, :math:`n*T_p, (n+1)*T_p`, :math:`B[n*T_p] > B[(n+1)*T_p]`,
which extends to:

.. math::
   B[n*T_p] > B[m*T_p], \forall n>m>0

Therefore the amount of data in the system's buffer increases every
period, and the system is unstable.

If however, there is enough remaining capacity in the system to
service the data in the buffer, i.e. :math:`R[T_p] >= B[T_p]`, then
:math:`B[2*T_p] = B[T_p]`. Similarly to above, since the system is
deterministic, for any two successive periods, :math:`n*T_p,
(n+1)*T_p`, :math:`B[n*T_p] = B[(n+1)*T_p]`.  This extends to:

.. math::
   B[n*T_p] = B[m*T_p], \forall m,n > 0

Therefore the buffer size does not grow between periods, and the
system is stable.

If we are only concerned with system stability, we do not need to
calculate :math:`R`, and can instead infer system stability by
comparing the values of the buffer at any two period-offset times
during the steady-state operation of the system (:math:`t >= T_p`).
This means that system stability check can resolve to :math:`B[T_p] ==
B[2*T_p]`.

Comparison with NC/RTC
~~~~~~~~~~~~~~~~~~~~~~

To show how our analysis techniques compare to other available
methods, we developed our tools to allow us to analyze the input
system using Network Calculus/Real-Time Calculus techniques as well as
our own.  Using these capabilities, we can directly compare the
analysis results to each other, and then finally compare both results
to the measurements from the actual system.

Taking the results from our published work, where our methods
predicted a buffer size of 64000 bits / 8000 bytes, we show that
Network Calculus predicts a required buffer size of 3155000 bits.

.. figure:: /images/results/maren_namek_data.png
   :align: center

   Analysis of the system with our tools.
	
.. figure:: /images/results/nc_namek_data.png
   :align: center

   Network-Calculus based analysis of the system.

We developed software which produces data according to a supplied
input profile and configured the system's network to provide the
bandwidth profile described in the system configuration profile.
Using this experimental infrastructure, we were able to measure the
transmitted traffic profile, the received traffic profile, the latency
experienced by the data, and the transmitter's buffer requirements.
The results are displayed in the table below:

+---------------------+--------------+-------------------------------+
|                     | Predicted    | Measured (:math:`\mu,\sigma`) |
+=====================+==============+===============================+
| Buffer Delay (s)    | 0.0625       | (0.06003 , 0.00029)           |
+---------------------+--------------+-------------------------------+
| Time of Delay (s)   | 3.0          | (2.90547 , 0.00025)           |
+---------------------+--------------+-------------------------------+
| Buffer Size (bytes) | 8000         | (7722.59 , 36.94)             |
+---------------------+--------------+-------------------------------+

	
Analysis of TDMA Scheduling
~~~~~~~~~~~~~~~~~~~~~~~~~~~

So far, the description of the system provided network service profile
(:math:`p[t]=y`), has been abstracted as simply the available
bandwidth as a function of time integrated to produce the amount of
data serviced as a function of time.  In order to more precisely model
the system, a network medium channel access protocol must be
integrated into the abstract system provided profile.  TDMA is such a
protocol which assigns to each node one or more time slots in a
repeating period during which only the selected node is allowed to
transmit.  We show how to model such a protocol and extend the
abstract system network profile to include the model of the TDMA
channel access protocol.

As an example TDMA system which benefits from our analysis techniques,
consider an application platform provided by a fractionated satellite
cluster.  A fractionated satellite cluster consists of many small
satellites that may each have different hardware, computing, and
communications capabilities.  These capabilities are provided to
distributed components of the satellite cluster's applications.  Such
a system has the combined challenges of (1) being expensive to
develop, test, and deploy, (2) being very difficult to repair or
replace in the event of failure, and (3) having to support
mixed-criticality and possibly multiple levels of security
applications.  For this system, the network between these satellites
is a precious resource shared between each of the applications'
components in the cluster.  To ensure the stability of the network
resources, each satellite has a direct connection to every other
satellite and is assigned a slot in the TDMA schedule during which the
satellite may transmit.  Each TDMA slot has a sinusoidally
time-varying bandwidth profile which may differ from the other TDMA
slot bandwidth profiles.  The time-varying profile of the slot
bandwidth comes from the coupling between the radios' inverse-squared
bandwidth-as-a-function-of-distance and the satellites' sinusoidal
distance-as-a-function-of-orbital-position.

Such a system and applications necessitates design-time guarantees
about resource utilization and availability.  Applications which
utilize the satellite network need assurances that the network
resources they require during each part of the orbital period will be
satisfied.  To provide these assurances, we provide the application
developers and system integrators the ability to specify and analyze
the network profiles as (possibly periodic) functions of time.
Furthermore, the requirement for accurate predictions necessitates the
incorporation of the TDMA scheduling and bandwidth profiling into the
network modeling and analysis tools.

TDMA schedules can be described by their period, their number of
slots, and the bandwidth available to each slot as a function of time.
For simplicity of explanation, we assume that each node only gets a
single slot in the TDMA period and all slots have the same length, but
the results are valid for all static TDMA schedules.  Note that each
slot still has a bandwidth profile which varies as a function of time
and that each slots may have a different bandwidth profile.  In a
given TDMA period (:math:`T`), the node can transmit a certain number
of bits governed by its slot length (:math:`t_{slot}`) and the slot's
available bandwidth (:math:`bw_{slot}`).  During the rest of the TDMA
period, the node's available bandwidth is :math:`0`.  This scheduling
has the effect of amortizing the node's slot bandwidth into an
effective bandwidth of :math:`bw_{effective} = bw_{slot} *
\dfrac{t_{slot}}{T}`.  The addition of the TDMA scheduling can affect
the buffer and delay calculations, based on the slot's bandwidth, the
number of slots, and the slot length.  The maximum additional delay is
:math:`\Delta_{delay} = T - t_{slot}`, and the maximum additional
buffer space is :math:`\Delta_{buffer} = \Delta_{delay} *
bw_{effective}`.  These deviations are shown in
Figure~\ref{fig:deviation}.  Clearly, :math:`\Delta_{delay}` is
bounded by :math:`T` and :math:`\Delta_{buffer}` is governed by
:math:`t_{slot}`.  Therefore, because :math:`t_{slot}` is dependent on
:math:`T`, minimizing :math:`T` minimizes both the maximum extra delay
and maximum extra buffer space.


Compositional Analysis
~~~~~~~~~~~~~~~~~~~~~~

We have implemented min-plus calculus based compositional operations
for the network profiles which allow us to compose and decompose
systems based on functional components.  For network flows, this means
we can analyze flows individually to determine per-flow performance
metrics or we can aggregate flows together to determine aggregate
performance.

The composition is priority based, with each flow receiving a unique
priority.  This priority determines the oder in which the flows are
individually analyzed, with the system's remaining capacity being
provided to the flow with the next highest priority.  This is similar
to the modular performance analysis provided by Real-Time Calculus.

The basis for this priority-based interaction is the QoS management
provided by many different types of networking infrastructure.
DiffServ's DSCP provides one mechanism to implement this
priority-based transmission and routing.


Delay Analysis
~~~~~~~~~~~~~~

When dealing with queueing systems (esp. networks) where precise
design-time guarantees are required, the delay in the links of the
network must be taken into account.

The delay is modeled as a continuous function of latency (seconds)
versus time.  In the profiles, the latency is specified discretely as
:math:`(time, latency)` pairs, and is interpolated linearly between
successive pairs.

Using these latency semantics, the delay convolution of a profile
becomes

.. math::
   r[t + \delta[t]] = l[t]

Where

* :math:`l[t]` is the *link* profile describing the data as a function
  of time as it enters the link
* :math:`\delta[t]` is the *delay* profile describing the latency as a
  function of time on the link
* :math:`r[t]` is the *received* profile describing the data as a
  function of time as it is received at the end of the link

When analyzing delay in a periodic system, it is important to
determine the effects of delay on the system's periodicity.  We know
that the period of the periodic profiles is defined by the time
difference between the start of the profile and the end of the
profile.  Therefore, we can show that if the time difference between
the **start time** of the *received* profile and the **end time** of
the *received* profile is the same as the **period** of the *link*
profile, the periodicity of the profile is unchanged.

* :math:`T_p` is the period of the *link* profile
* :math:`r[t + \delta[t]]` is the beginning of the *received* profile
* :math:`r[(t + T_p) + \delta[(t + T_p)]]` is the end of the
  *received* profile
    

We determine the condition for which :math:`(t_{end}) - (t_{start}) =
T_p`:

.. math::
   (T_p + t + \delta[T_p + t]) - (t + \delta[t]) &= T_p \\
   T_p + \delta[T_p + t] - \delta[t] &= T_p \\
   \delta[T_p + t] - \delta[t] &= 0\\
   \delta[T_p + t] &= \delta[t]

From this we determine that the periodicitiy of the profile is
unchanged *iff* the profile is period-continuous, i.e. if the latency
at the end of the profile is the same as the latency at the beginning
of the profile.

Routing Analysis
~~~~~~~~~~~~~~~~

By incorporating both the latency analysis with the compositional
operations we developed, we can perform system-level analysis of flows
which are routed by nodes of the system.  In this paradigm, nodes can
transmit/receive their own data, i.e. they can host applications which
act as data sources or sinks, as well as acting as routers for flows
from and to other nodes.  To make such a system amenable to analysis
we must ensure that we know the routes the flows will take at design
time, i.e. the routes in the network are static and known or
calculable.  Furthermore, we must, for the sake of flow composition as
decribed above, ensure that each flow has a priority that is unique
within the network which governs how the transmitting and routing
nodes handle the flow's data.

We have extended our network analysis tool to support such system
analysis by taking as input:

* the flows in the network
* the provided service of each link in the network
* the network configuration specifying the nodes in the network and
  the routes

where a flow is defined by:

* ID of the source node
* ID of the destination node
* Priority of the flow
* flow profile, i.e. bandwidth vs time

We can then run the following algorithm to iteratively analyze the
flows and the system:

.. figure:: /images/results/algorithm.svg
	    :height: 600px
	    :width: 600px

In this algorithm, the remaining capacity of the node is provided to
each profile with a lower priority iteratively.

.. _run_time:

Run Time Results
----------------

Middleware-integrated Measurement, Detection, and Enforcement
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

We have implemented these features based on our design-time results

* Traffic generators according to profile generated into sender code
* Receiver service according to profile generated into receiver code
* Measurement of output traffic on sender side and input traffic on
  server side generated into code
* Detection of anomalous sending on sender side
* Mitigation of anoumalous sending on sender side
* Detection of anomalous sending on receiver side
* Push back to sender middleware through out-of-band channel for
  anomaly detection on server side

Have shown experimentally that, for example, a server side buffer size
of 400000 bits, which would normally grow to 459424 bits because of
excessive data pumps on the sender side, is kept to 393792 by
utilizing this out-of-band channel and secure middleware.
    
