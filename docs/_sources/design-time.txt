.. _design_time:

Design Time Results
===================

These results provide a methodology and a means for application
developers and system integrators to determine conservative, precise,
tightly bounded performance metrics for distributed networked
applications and systems at design time.  The contributions of this
work are broken into sections by topic:

* :ref:`assumptions`
* :ref:`math_foundation`
* :ref:`periodic_analysis_proof`
* :ref:`nc_comparison`
* :ref:`tdma_analysis`
* :ref:`compositional_analysis`
* :ref:`delay_analysis`
* :ref:`routing_analysis`

.. _assumptions:

Assumptions Required for Analysis and Performance Prediction
------------------------------------------------------------

As with any type of system modeling and analysis paradigm, it is
important to remain aware of the types of systems the
modeling/analysis is applicable to, the requirements imposed on the
system by the model, and any edge cases or scenarios where the
analysis or modeling paradigm breaks down.

A key assumption and thus requirement of our modeling and analysis
framework is a system-wide synchronized clock which all nodes use.
This is required for the composition of profiles as they traverse the
network and are routed through nodes.  This assumption restricts the
types of systems for which our analysis can be most useful, but is not
a critical hindrance, as many such critical systems, e.g. satellite
constellations or UAVs have GPS synchronized clocks, which provide
such a foundation.


.. _math_foundation:

Mathematical Foundation for Precise Performance Prediction
----------------------------------------------------------

To model the network capability of the system and the application
traffic patterns, we have developed a network modeling paradigm
similar to Network Calculus' traffic arrival curves and traffic shaper
service curves.

Similarly to Network Calculus' arrival curves and service curves, our
network profiles model how the network performance or application
traffic generation changes with respect to time.  Whereas Network
Calculus' modeling transforms application data profiles and network
service profiles into min and max curves for data received vs. size of
time-window, our models take a simpler, deterministic approach which
models exactly the data generated by the application and the data
which could be sent through the network, allowing our performance
metrics to be more precise.  Specifically, the bandwidth that the
network provides on a given communication link is specified as a time
series of scalar bandwidth values. Here, bandwidth is defined as data
rate, i.e. bits per second, over some averaging interval.  This
bandwidth profile can then be time-integrated to determine the maximum
amount of data throughput the network link could provide over a given
time.  The bandwidth profile for the application traffic similarly can
be time-integrated to determine the amount of data that the
application attempts to send on the network link as a function of
time.

Having time-integrated the bandwidth profiles to obtain data vs. time
profiles that the application requires and that the system provides,
we can use convolution (:math:`\otimes`) on these two profiles to
obtain the transmitted link data profile as a function of discrete
time. The convolution we define on these profiles borrows concepts
from the min-plus calculus used in Network Calculus, but does not use
a sliding-window and instead takes the transformed minimum of the
profiles. For a given application data generation profile,
:math:`r[t]`, and a given system link capacity profile :math:`p[t]`,
where :math:`t\in\mathbb{N}`, the link transmitted data profile
:math:`l[t]` is given by the convolution equation
:eq:`convolution`. The difference :math:`(p[t-1] - l[t-1])` represents
the difference between the amount of data that has been transmitted on
the link :math:`(l[t-1])` and the data that the link could have
transmitted at full utilization :math:`(p[t-1])`. As demonstrated by
the convolution equation, :math:`\forall t : l[t] \le r[t]`, which is
the relation that, without lower-layer reliable transport, the link
cannot transmit more application data for the application than the
application requests as there will be packetization and communication
header overhead as well.  The buffer and delay equations
:eq:`convolution` use the output of the convolution with the input
profile to predict the minimum required buffer size for lossless
tranmission and the maximum delay experienced by the transmitted data,
respectively.  A representative convolution example is shown below for
reference.  These functions are implemented as:
:func:`networkProfile.Profile.Convolve`,
:func:`networkProfile.Profile.CalcBuffer`,
and :func:`networkProfile.Profile.CalcDelay`.  

.. math::
   y=l[t] &= (r \otimes p)[t] \\
   &= min( r[t] , p[t] - (p[t-1] - l[t-1]) )\\
   \text{buffer}&= sup\{r[t] - l[t] : t \in \mathbb{N}\}\\
   \text{delay} &= sup\{l^{-1}[y]-r^{-1}[y] : y \in \mathbb{N}\}
   :label: convolution

.. figure:: /images/results/convolution.png
   :align: center

Given that the required data profile and system data service profile
are periodic, we must determine the periodicity of the output
profile.  If we can show that the output profile is similarly
periodic, then we can show that the system is stable.  First, let us
look at the profile behavior over the course of its first two periods
of activity.

We will examine two systems, *system (1)* and *system (2)*.  Firstly,
examine *(1)*, shown below (note: you can click on the images to open
them in a larger format):

+---------------------------------------------------+-----------------------------------------------------+
| System *(1)* Bandwidth for 1 Period               | System *(1)* Data for 1 Period                      |
+===================================================+=====================================================+
| .. image:: /images/results/1-period-system-bw.png | .. image:: /images/results/1-period-system-data.png |
|    :height: 200                                   |    :height: 200                                     |
+---------------------------------------------------+-----------------------------------------------------+

+---------------------------------------------------+-----------------------------------------------------+
| System *(1)* Bandwidth for 2 Periods              | System *(1)* Data for 2 Periods                     |
+===================================================+=====================================================+
| .. image:: /images/results/2-period-system-bw.png | .. image:: /images/results/2-period-system-data.png |
|    :height: 200                                   |    :height: 200                                     |
+---------------------------------------------------+-----------------------------------------------------+

We notice that for this example system, the second period output
profile is not an exact copy of the first (most easily seen by
examining the bandwidth plots), and yet the required buffer size is
still the same as it was when analyzing the system over one period.
Furthermore, by running the analysis over even larger number of
periods, we can determine (not plotted here for space and
readability), that the predicted buffer size does not change no matter
how many periods we analyze for this system.

Let us look at a system where this is not the case before we begin the
analysis of such system characteristics.

+-----------------------------------------------------+-------------------------------------------------------+
| System *(2)* Bandwidth for 1 Period                 | System *(2)* Data for 1 Period                        |
+=====================================================+=======================================================+
| .. image:: /images/results/1-period-unstable-bw.png | .. image:: /images/results/1-period-unstable-data.png |
|    :height: 200                                     |    :height: 200                                       |
+-----------------------------------------------------+-------------------------------------------------------+

+-----------------------------------------------------+-------------------------------------------------------+
| System *(2)* Bandwidth for 2 Periods                | System *(2)* Data for 2 Periods                       |
+=====================================================+=======================================================+
| .. image:: /images/results/2-period-unstable-bw.png | .. image:: /images/results/2-period-unstable-data.png |
|    :height: 200                                     |    :height: 200                                       |
+-----------------------------------------------------+-------------------------------------------------------+

Notice in system *(2)*, the first period analysis predicted the same
buffer size and delay as system *(1)*, but when analyzing two periods
the predicted buffer size changed.  Clearly the behavior of the system
is changing between these two periods.  If we continue to analyze more
periods of system *(2)*, as we did with system *(1)*, we'll find the
unfortunate conclusion that the predicted buffer size increases with
every period we add to the analysis.

We have discovered a system level property that can be calculated from
these profiles, but we must determine what it means and how it can be
used.  First, we see that in system *(1)*, the predicted required
buffer size does not change regarless of the number of periods over
which we analyze the system.  Second, we see that for system *(2)*,
the predicted required buffer size changes depending on how many
periods of activity we choose for our analysis window.  Third, we see
that the second period of system *(2)* contains the larger of the two
predicted buffer sizes.  These observations (with our understanding of
deterministic periodic systems) lead us to the conclusion: system
*(2)* can no longer be classified as periodic, since its behavior is
not consistent between its periods.  Furthermore, because the required
buffer size predicted for system system *(2)* continually increases,
we can determine that the system is in fact *unstable* due to
unbounded buffer growth.  

.. _periodic_analysis_proof:

Proving the Minimum Analysis for System Stability
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Let us now formally prove the assertion about system periodicity and
stability which has been stated above.  We will show that our analysis
results provide quantitative measures about the behavior of the system
and we will determine for how long we must analyze a system to glean
such behaviors.

Consider a deterministic queuing system providing a data service
function :math:`S` to input data flow :math:`I` to produce output data
flow :math:`O`.  At any time :math:`t`, the amount of data in the
system's buffer is given by :math:`B_t`.  After servicing the input
flow, the system has a remaining capacity function :math:`R`.

* :math:`S[t]` : the service function of the system, data service
  capacity versus time
* :math:`I[t]` : the input data flow to the system, data versus time
* :math:`O[t]` : the output data flow from the system, data versus time
* :math:`B[t]` : the amount of data in the system's buffer at time
  :math:`t`, i.e. :math:`I[t]-O[t]`
* :math:`R[t]` : the remaining service capacity of the system after
  servicing :math:`I`, i.e. :math:`S[t] - O[t]`

Because :math:`S` and :math:`I` are deterministic and periodic, they
do not change from period to period, i.e. given the period :math:`T_I`
of :math:`I`, :math:`\forall t,n : I[t] = I[t + n*T_I]`.  Similarly,
given the period :math:`T_S` of :math:`S`, :math:`\forall t,n : S[t] =
S[t + n*T_S]`.

We can determine the hyperperiod of the system as the :func:`utils.lcm` of input
flow period and the service function period, :math:`T_p =
lcm(T_S,T_I)`.

At the start of the system, :math:`t=0`, the system's buffer is empty,
i.e.  :math:`B[0] = 0`.  Therefore, the amount of data in the buffer at
the end of the first period, :math:`t=T_p`, is the amount of data that
entered the system on input flow :math:`I` but was not able to be
serviced by :math:`S`.  At the start of the next period, this data
will exist in the buffer.  Consider the scenario that the system's
remaining capacity :math:`R` is less than the size of the buffer,
i.e. :math:`R[T_p] < B[T_p]`.  In this scenario, clearly,
:math:`B[2*T_p] > B[T_p]`, i.e. there will be more data in the buffer
at the end of the second period than there was at the end of the first
period.  Since the system is deterministic, for any two successive
periods, :math:`n*T_p` and :math:`(n+1)*T_p`, :math:`B[n*T_p] > B[(n+1)*T_p]`,
which extends to:

.. math::
   B[m*T_p] > B[n*T_p], \forall m>n>0

Therefore the amount of data in the system's buffer increases every
period, and the system is unstable.

If however, there is enough remaining capacity in the system to
service the data in the buffer, i.e. :math:`R[T_p] >= B[T_p]`, then
:math:`B[2*T_p] = B[T_p]`. Similarly to above, since the system is
deterministic, for any two successive periods, :math:`n*T_p` and
:math:`(n+1)*T_p`, :math:`B[(n+1)*T_p] = B[n*T_p]`.  This extends to:

.. math::
   B[m*T_p] = B[n*T_p], \forall m,n > 0

Therefore the buffer size does not grow between periods, and the
system is stable.

If we are only concerned with system stability, we do not need to
calculate :math:`R`, and can instead infer system stability by
comparing the values of the buffer at any two period-offset times
during the steady-state operation of the system (:math:`t >= T_p`).
This means that system stability check can resolve to :math:`B[2*T_p]
== B[T_p]`.  This comparison abides by the conditions above, with
:math:`m=2` and :math:`n=1`.  Checking for system stability occurs in
:func:`analysis.analyze`.

.. _nc_comparison:
      
Comparison with NC/RTC
----------------------

To show how our analysis techniques compare to other available
methods, we developed our tools to allow us to analyze the input
system using Network Calculus/Real-Time Calculus techniques as well as
our own.  Using these capabilities, we can directly compare the
analysis results to each other, and then finally compare both results
to the measurements from the actual system.  The convenience function
to generate a NC-based profile from our profile model is implemented
in :func:`networkProfile.Profile.ConvertToNC`.

Taking the results from our published work, where our methods
predicted a buffer size of 64000 bits / 8000 bytes, we show that
Network Calculus predicts a required buffer size of 3155000 bits.

.. figure:: /images/results/maren_namek_bw.png
   :align: center
   :height: 400px
   :width: 400px

   Bandwidth profile describing the system and application.

.. figure:: /images/results/maren_namek_data.png
   :align: center
   :height: 400px
   :width: 400px

   Analysis of the system with our tools.
	
.. figure:: /images/results/nc_namek_data.png
   :align: center
   :height: 400px
   :width: 400px

   Network-Calculus based analysis of the system.

The major drawback for Network Calculus that our work aims to solve is
the disconnect from the real system that stems from using an approach
based on time-window analysis.  Such an approach leads to dramatically
under-approximating the capacity of the network while simultaneously
over-approximating the utilization of the network, since a known drop
in network performance which is expected and handled by the
application cannot be accurately modeled.  In our case, the system is
using a system profile which can service data during the period from
:math:`0\le t\le 7` seconds with a period of 10 seconds.  The
application is designed around this constraint and only produces data
during that interval.  Because our technique directly compares when
the application produces data to when the system can service the data,
we are able to derive more precise performance prediction metrics than
Network Calculus, which compares the 3 seconds of system downtime to
the 3 seconds of maximum application data production.

We developed software which produces data according to a supplied
input profile and configured the system's network to provide the
bandwidth profile described in the system configuration profile.
Using this experimental infrastructure, we were able to measure the
transmitted traffic profile, the received traffic profile, the latency
experienced by the data, and the transmitter's buffer requirements.
The results are displayed in the table below:

+---------------------+--------------+-------------------------------+
|                     | Predicted    | Measured (:math:`\mu,\sigma`) |
+=====================+==============+===============================+
| Buffer Delay (s)    | 0.0625       | (0.06003 , 0.00029)           |
+---------------------+--------------+-------------------------------+
| Time of Delay (s)   | 3.0          | (2.90547 , 0.00025)           |
+---------------------+--------------+-------------------------------+
| Buffer Size (bytes) | 8000         | (7722.59 , 36.94)             |
+---------------------+--------------+-------------------------------+

.. _tdma_analysis:
	
Analysis of TDMA Scheduling
---------------------------

So far, the description of the system provided network service profile
(:math:`p[t]=y`), has been abstracted as simply the available
bandwidth as a function of time integrated to produce the amount of
data serviced as a function of time.  In order to more precisely model
the system, a network medium channel access protocol must be
integrated into the abstract system provided profile.  TDMA is such a
protocol which assigns to each node one or more time slots in a
repeating period during which only the selected node is allowed to
transmit.  We show how to model such a protocol and extend the
abstract system network profile to include the model of the TDMA
channel access protocol.

As an example TDMA system which benefits from our analysis techniques,
consider an application platform provided by a fractionated satellite
cluster.  A fractionated satellite cluster consists of many small
satellites that may each have different hardware, computing, and
communications capabilities.  These capabilities are provided to
distributed components of the satellite cluster's applications.  Such
a system has the combined challenges of (1) being expensive to
develop, test, and deploy, (2) being very difficult to repair or
replace in the event of failure, and (3) having to support
mixed-criticality and possibly multiple levels of security
applications.  For this system, the network between these satellites
is a precious resource shared between each of the applications'
components in the cluster.  To ensure the stability of the network
resources, each satellite has a direct connection to every other
satellite and is assigned a slot in the TDMA schedule during which the
satellite may transmit.  Each TDMA slot has a sinusoidally
time-varying bandwidth profile which may differ from the other TDMA
slot bandwidth profiles.  The time-varying profile of the slot
bandwidth comes from the coupling between the radios' inverse-squared
bandwidth-as-a-function-of-distance and the satellites' sinusoidal
distance-as-a-function-of-orbital-position.

Such a system and applications necessitates design-time guarantees
about resource utilization and availability.  Applications which
utilize the satellite network need assurances that the network
resources they require during each part of the orbital period will be
satisfied.  To provide these assurances, we provide the application
developers and system integrators the ability to specify and analyze
the network profiles as (possibly periodic) functions of time.
Furthermore, the requirement for accurate predictions necessitates the
incorporation of the TDMA scheduling and bandwidth profiling into the
network modeling and analysis tools.

TDMA schedules can be described by their period, their number of
slots, and the bandwidth available to each slot as a function of time.
For simplicity of explanation, we assume that each node only gets a
single slot in the TDMA period and all slots have the same length, but
the results are valid for all static TDMA schedules.  Note that each
slot still has a bandwidth profile which varies as a function of time
and that each slots may have a different bandwidth profile.  In a
given TDMA period (:math:`T`), the node can transmit a certain number
of bits governed by its slot length (:math:`t_{slot}`) and the slot's
available bandwidth (:math:`bw_{slot}`).  During the rest of the TDMA
period, the node's available bandwidth is :math:`0`.  This scheduling
has the effect of amortizing the node's slot bandwidth into an
effective bandwidth of :math:`bw_{effective} = bw_{slot} *
\dfrac{t_{slot}}{T}`.  The addition of the TDMA scheduling can affect
the buffer and delay calculations, based on the slot's bandwidth, the
number of slots, and the slot length.  The maximum additional delay is
:math:`\Delta_{delay} = T - t_{slot}`, and the maximum additional
buffer space is :math:`\Delta_{buffer} = \Delta_{delay} *
bw_{effective}`.  These deviations are shown below.  Clearly,
:math:`\Delta_{delay}` is bounded by :math:`T` and
:math:`\Delta_{buffer}` is governed by :math:`t_{slot}`.  Therefore,
because :math:`t_{slot}` is dependent on :math:`T`, minimizing
:math:`T` minimizes both the maximum extra delay and maximum extra
buffer space.

+---------------------------------------------------+-----------------------------------------------------+
| In-Phase TDMA profile vs abstract                 | Out-of-Phase TDMA Profile vs abstract               |
+===================================================+=====================================================+
| .. image:: /images/results/tdma_phase0.png        | .. image:: /images/results/tdma_phase1.png          |
|    :height: 200                                   |    :height: 200                                     |
+---------------------------------------------------+-----------------------------------------------------+

Following from this analysis, we see that if: (1) the TDMA effective
bandwidth profile is provided as the abstract system network service
profile, and (2) the TDMA period is much smaller than the duration of
the shortest profile interval; then the system with explicit modeling
of the TDMA schedule has similar predicted application network
characteristics as the abstract system.  Additionally, the maximum
deviation formulas derived above provide a means for application
developers to analyze the their application on a TDMA system without
explicitly integrating the TDMA model into the system profile model.

.. _compositional_analysis:

Compositional Analysis
----------------------

Now that we have precise network performance analysis for aggregate
flows or singular flows on individual nodes of the network, we must
determine how best to compose these flows and nodes together to
analyze the overal system.  The aim of this work is to allow the flows
from each application to be analyzed separately from the other flows
in the network, so that application developers and system integrators
can derive meaningful perfomance predictions for specific
applications.  

We have implemented min-plus calculus based compositional operations
for the network profiles which allow us to compose and decompose
systems based on functional components.  For network flows, this means
we can analyze flows individually to determine per-flow performance
metrics or we can aggregate flows together to determine aggregate
performance.  Profile addition and subtraciton are implemented
in :func:`networkProfile.Profile.AddProfile` and
:func:`networkProfile.Profile.SubtractProfile`.  Using these functions
we can aggregate or separate flow profiles and service profiles.

The composition is priority based, with each flow receiving a unique
priority.  This priority determines the oder in which the flows are
individually analyzed, with the system's remaining capacity being
provided to the flow with the next highest priority.  This is similar
to the modular performance analysis provided by Real-Time Calculus.

The basis for this priority-based interaction is the QoS management
provided by many different types of networking infrastructure.
DiffServ's DSCP provides one mechanism to implement this
priority-based transmission and routing.

We are finalizing the design and code for tests which utilize the DSCP
bit(s) setting on packet flows to show that such priority-based
analysis techniques are correct for these types of systems.

.. _delay_analysis:

Delay Analysis
--------------

When dealing with queueing systems (esp. networks) where precise
design-time guarantees are required, the delay in the links of the
network must be taken into account.

The delay is modeled as a continuous function of latency (seconds)
versus time.  In the profiles, the latency is specified discretely as
:math:`(time, latency)` pairs, and is interpolated linearly between
successive pairs.

Using these latency semantics, the delay convolution of a profile
becomes

.. math::
   r[t + \delta[t]] = l[t]

Where

* :math:`l[t]` is the *link* profile describing the data as a function
  of time as it enters the link
* :math:`\delta[t]` is the *delay* profile describing the latency as a
  function of time on the link
* :math:`r[t]` is the *received* profile describing the data as a
  function of time as it is received at the end of the link

When analyzing delay in a periodic system, it is important to
determine the effects of delay on the system's periodicity.  We know
that the period of the periodic profiles is defined by the time
difference between the start of the profile and the end of the
profile.  Therefore, we can show that if the time difference between
the **start time** of the *received* profile and the **end time** of
the *received* profile is the same as the **period** of the *link*
profile, the periodicity of the profile is unchanged.

* :math:`T_p` is the period of the *link* profile
* :math:`r[t + \delta[t]]` is the beginning of the *received* profile
* :math:`r[(t + T_p) + \delta[(t + T_p)]]` is the end of the
  *received* profile
    

We determine the condition for which :math:`(t_{end}) - (t_{start}) =
T_p`:

.. math::
   (T_p + t + \delta[T_p + t]) - (t + \delta[t]) &= T_p \\
   T_p + \delta[T_p + t] - \delta[t] &= T_p \\
   \delta[T_p + t] - \delta[t] &= 0\\
   \delta[T_p + t] &= \delta[t]

From this we determine that the periodicitiy of the profile is
unchanged *iff* the profile is period-continuous, i.e. if the latency
at the end of the profile is the same as the latency at the beginning
of the profile.

The profile delay operation is implemented in
:func:`networkProfile.Profile.Delay`.

.. _routing_analysis:

Routing Analysis
----------------

By incorporating both the latency analysis with the compositional
operations we developed, we can perform system-level analysis of flows
which are routed by nodes of the system.  In this paradigm, nodes can
transmit/receive their own data, i.e. they can host applications which
act as data sources or sinks, as well as acting as routers for flows
from and to other nodes.  To make such a system amenable to analysis
we must ensure that we know the routes the flows will take at design
time, i.e. the routes in the network are static and known or
calculable.  Furthermore, we must, for the sake of flow composition as
decribed above, ensure that each flow has a priority that is unique
within the network which governs how the transmitting and routing
nodes handle the flow's data.

We have extended our network analysis tool to support such system
analysis by taking as input:

* the flows in the network
* the provided service of each node in the network
* the network configuration specifying the nodes in the network and
  the routes in the network

where a flow is defined by (see
:func:`networkProfile.Profile.ParseHeader`):

* ID of the source node
* ID of the destination node
* Priority of the flow
* flow properties vs time profile, see
  :func:`networkProfile.Profile.ParseEntriesFromLine`

and a route is specified as a list of node IDs starting with the
source node ID and ending with the destination node ID.  Any flows
which have the respective source and destination IDs must travel along
the path specified by the respective route.  The route and the toplogy
are implemented in :class:`networkConfig.Route` and
:class:`networkConfig.Topology`, and the network configuration
specification is found in :class:`networkConfig.Config`.

We can then run the following algorithm to iteratively analyze the
flows and the system:

.. figure:: /images/results/algorithm.svg
	    :height: 600px
	    :width: 600px

In this algorithm, the remaining capacity of the node is provided to
each profile with a lower priority iteratively.

We have implmented these functions for statically routed network
analysis into our tool, which automatically parses the flow profiles,
the network configuration and uses the algorithm and the implemented
mathematics to iteratively analyze the network.  Analytical results
for example systems will be provided when the experimental results can
be used as a comparison.  The analysis algorithm is implemented by
:func:`analysis.main`.

We are finishing the design and development of code which will allow
us to run experiments to validate our routing analysis results.  They
will be complete in the next two weeks.

